{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table style=\"margin: auto; background-color: white;\">\n",
        "    <tr>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1R6PphT9jmd2vikODFPf6cW54QtZ29o2a' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1R6PphT9jmd2vikODFPf6cW54QtZ29o2a' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1R6PphT9jmd2vikODFPf6cW54QtZ29o2a' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "    </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "XqLZVV10-uOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TUTORIAL 3\n",
        "\n",
        "Bem-vindo! Neste tutorial você aprenderá sobre a interface de programação da plataforma **Flautim** e também como montar um experimento simples de classificação usando o dataset [MNIST](https://github.com/zalandoresearch/fashion-mnist).\n",
        "\n",
        "O código desse tutorial pode ser acessado em: [clique aqui](https://github.com/FutureLab-DCC/flautim_tutoriais/tree/main/TUTORIAL_3).\n",
        "\n",
        "\n",
        "Vamos começar entendendo a interface de programação da **Flautim**, representada na figura abaixo. A **Flautim_api** é uma biblioteca modularizada que facilita a realização de experimentos de aprendizado de máquina, seja convencional/centralizado ou federado.\n",
        "\n",
        "Todo projeto **Flautim** precisa herdar essa biblioteca, que contém submódulos específicos para diferentes tecnologias (por exemplo, submódulos para PyTorch, TensorFlow, etc). Neste tutorial usaremos o submódulo para PyTorch.\n",
        "\n",
        "<div style=\"text-align: center;\"> <table style=\"margin: auto;\"> <tr> <td> <img src='https://drive.google.com/uc?export=view&id=1QOI4jWrwS979xhW_wlGzkPa2bMA-giuc' alt=\"Interface da plataforma Flautim\" width=\"800\" /> </td> </tr> </table> </div>\n",
        "\n",
        "\n",
        "Dentro de cada submódulo existem três componentes principais (classes):\n",
        "\n",
        "**1. Dataset:** é utilizado para representar os dados do experimento. Esta classe pode ser reutilizada em diversos experimentos e com diferentes modelos, sendo o componente mais versátil e reutilizável. Os usuários podem importar os dados de diversas fontes, como arquivos locais ou bases de dados online, desde que a classe Dataset seja herdada.\n",
        "\n",
        "**2. Model:** representa qualquer conjunto de parâmetros treináveis dentro do projeto. Ela permite a aplicação de técnicas de aprendizado de máquina por meio de treinamento desses parâmetros. No caso de PyTorch, a classe herda a nn.Module, que define a estrutura e os parâmetros treináveis do modelo.\n",
        "\n",
        "**3. Experiment:** define o ciclo de treinamento e validação. Existem dois tipos principais de experimentos: o experimento centralizado, que segue o fluxo\n",
        "convencional de aprendizado de máquina, e o experimento federado, adaptado para\n",
        "aprendizado federado. Esta classe inclui duas funções principais, um loop de\n",
        "treinamento e um loop de validação, que realizam a atualização dos parâmetros e\n",
        "cálculo das métricas de custo, respectivamente.\n",
        "\n",
        "Além desses três componentes principais, há também um módulo chamado Common. Este módulo fornece acesso a classes essenciais para o gerenciamento de dados e monitoramento do treinamento.\n",
        "\n",
        "\n",
        "Com essa visão geral, você está pronto para começar montar seus próprios experimentos. Vamos ao passo a passo!"
      ],
      "metadata": {
        "id": "5HacTzoG_xm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Passo 1: Criando o dataset que será usado no experimento\n",
        "\n",
        "Um conjunto de dados no Flautim é acessado por um arquivo .py que deve conter uma classe que herda de Dataset.\n",
        "\n",
        "**Exemplo: Implementando a Classe MNISTDataset**\n",
        "\n",
        "O código abaixo implementa uma classe MNISTDataset utilizando o dataset MNIST para resolver um problema de classificação."
      ],
      "metadata": {
        "id": "ChJrIZCYeTeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flautim.pytorch.Dataset import Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class MNISTDataset(Dataset):\n",
        "\n",
        "    def __init__(self, FM_Normalization, EVAL_Transforms, TRAIN_Transforms, partition, **kwargs):\n",
        "\n",
        "        name = kwargs.get('name', 'MNIST')\n",
        "\n",
        "        super(MNISTDataset, self).__init__(name, **kwargs)\n",
        "\n",
        "        self.FM_Normalization = FM_Normalization\n",
        "        self.EVAL_Transforms = EVAL_Transforms\n",
        "        self.TRAIN_Transforms = TRAIN_Transforms\n",
        "\n",
        "        self.feature_name = kwargs.get(\"feature_name\", 'image')\n",
        "        self.split = kwargs.get(\"split_data\", True)\n",
        "\n",
        "        if self.split:\n",
        "            partition = partition.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "        self.train_partition = partition[\"train\"]\n",
        "        self.test_partition = partition[\"test\"]\n",
        "\n",
        "    def dataloader(self, validation = False):\n",
        "        tmp = self.validation() if validation else self.train()\n",
        "        return DataLoader(tmp, batch_size = self.batch_size, num_workers = 1)\n",
        "\n",
        "\n",
        "\n",
        "    def apply_train_transforms(self, batch):\n",
        "        \"\"\"Apply transforms to the partition from FederatedDataset.\"\"\"\n",
        "        batch[self.feature_name] = [self.TRAIN_Transforms(img) for img in batch[self.feature_name]]\n",
        "        return batch\n",
        "\n",
        "\n",
        "    def  apply_eval_transforms(self, batch):\n",
        "        \"\"\"Apply transforms to the partition from FederatedDataset.\"\"\"\n",
        "        batch[self.feature_name] = [self.EVAL_Transforms(img) for img in batch[self.feature_name]]\n",
        "        return batch\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        return self.train_partition.with_transform(self.apply_train_transforms)\n",
        "\n",
        "\n",
        "    def validation(self):\n",
        "\n",
        "        return self.test_partition.with_transform(self.apply_eval_transforms)"
      ],
      "metadata": {
        "id": "Hz5LGFkzc9CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Passo 2: Criando o modelo que será usado no experimento\n",
        "\n",
        "Agora, vamos criar a classe que implementa o modelo. Essa classe deve herdar da classe Model.\n",
        "\n",
        "\n",
        "**Exemplo: Implementando a Classe MNISTModel**\n",
        "\n",
        "A classe Net implementa uma rede neural convolucional baseada na rede LeNet, com as seguintes camadas:\n",
        "* Uma camada convolucional com entrada de 1 canal, saída de 6 canais e kernel de tamanho 5×5;\n",
        "* Uma camada de Max Pooling com janela 2 e stride 2;\n",
        "* Uma segunda camada convolucional com entrada de 6 canais, saída de 16 canais e kernel de tamanho 5×5;\n",
        "* Uma camada totalmente conectada com 120 neurônios;\n",
        "* Uma camada totalmente conectada com 84 neurônios;\n",
        "* Uma camada de saída totalmente conectada com número de neurônios igual ao número de classes do problema\n",
        "\n",
        "Essa classe pode ser incluída, por exemplo, em um arquivo MnistModel.py"
      ],
      "metadata": {
        "id": "6EbqONNwmV3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X51g0zkU-ikV"
      },
      "outputs": [],
      "source": [
        "from flautim.pytorch.Model import Model\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MNISTModel(Model):\n",
        "    def __init__(self, context, num_classes: int, **kwargs) -> None:\n",
        "        super(MNISTModel, self).__init__(context, name = \"MNIST\", version = 1, id = 1, **kwargs)\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Passo 3: Criando o experimento\n",
        "\n",
        "Por fim, será criado o experimento, isto é, uma classe que implementa os loops de treinamento e validação do modelo MNISTModel no dataset MNISTDataset. Para isso, precisamos criar dois arquivos .py, o run.py (que deve ter obrigatoriamente esse nome) e o .py responsável por implementar o experimento, descritos a seguir:\n",
        "\n",
        "**1. Arquivo run.py:**\n",
        "\n",
        "* Esse arquivo é o ponto de entrada de todo experimento Flautim, pois é ele\n",
        "que deve iniciar a classe do experimento e também um modelo e um Dataset.\n",
        "\n",
        "**2. Arquivo .py do experimento:**\n",
        "\n",
        "* Esse arquivo deve conter uma classe que implemente os métodos de treinamento (training_loop) e validação (evaluation_loop) do modelo. Essa classe deve herdar da classe Experiment.\n",
        "\n",
        "Esse tutorial cobrirá dois tipos de experimentos, um experimento centralizado e outro descentralizado. Portanto, o passo 3 será dividido entre esses dois cenários."
      ],
      "metadata": {
        "id": "_TF_5DkrmXwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Passo 3.1: Experimento centralizado\n",
        "\n",
        "**Implementando a Classe MNISTExperiment**\n",
        "\n",
        "No código abaixo, a classe MNISTExperiment foi criada no modo centralizado com seus métodos training_loop e evaluation_loop para treinar e testar a rede neural. Esses métodos retornam o valor da função de perda e a acurácia de treinamento e de validação."
      ],
      "metadata": {
        "id": "pnc_5yyYxJUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flautim.pytorch.centralized.Experiment import Experiment\n",
        "\n",
        "import flautim as fl\n",
        "import flautim.metrics as flm\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "\n",
        "class MNISTExperiment(Experiment):\n",
        "    def __init__(self, model, dataset, context, **kwargs):\n",
        "        super(MNISTExperiment, self).__init__(model, dataset, context, **kwargs)\n",
        "\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "        self.lr = kwargs.get('lr', 0.001)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        self.epochs = kwargs.get('epochs', 30)\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "    def training_loop(self, data_loader):\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        yhat, y_real = [], []\n",
        "\n",
        "        for batch in data_loader:\n",
        "            images = batch[\"image\"]\n",
        "            labels = batch[\"label\"]\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(images.to(self.device))\n",
        "            loss = self.criterion(outputs, labels.to(self.device))\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            yhat.append(predicted.detach().cpu())\n",
        "            y_real.append(labels.detach().cpu())\n",
        "\n",
        "        accuracy = self.metrics.ACCURACY(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        accuracy_2 = self.metrics.ACCURACY_2(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        avg_trainloss = running_loss / len(data_loader)\n",
        "        return float(avg_trainloss), {'ACCURACY': accuracy, 'ACCURACY_2': accuracy_2}\n",
        "\n",
        "    def validation_loop(self, data_loader):\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        yhat, y_real = [], []\n",
        "\n",
        "        loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in data_loader:\n",
        "                images = batch[\"image\"].to(self.device)\n",
        "                labels = batch[\"label\"].to(self.device)\n",
        "                outputs = self.model(images)\n",
        "                loss += self.criterion(outputs, labels).item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                yhat.append(predicted.detach().cpu())\n",
        "                y_real.append(labels.detach().cpu())\n",
        "\n",
        "        accuracy = self.metrics.ACCURACY(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        accuracy_2 = self.metrics.ACCURACY_2(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        loss = loss / len(data_loader)\n",
        "\n",
        "        return float(loss), {'ACCURACY': accuracy, 'ACCURACY_2': accuracy_2}"
      ],
      "metadata": {
        "id": "md90drlNmZqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementando o run.py para realização de um experimento centralizado**\n",
        "\n",
        "**1. Upload do Conjunto de Dados:**\n",
        "\n",
        "* *Arquivo Local:* Se o seu conjunto de dados for um arquivo (por exemplo, CSV, NPZ, etc.), faça o upload para a plataforma e carregue-o usando o caminho ./data/nomedoarquivo.\n",
        "\n",
        "* *URL:* Se o conjunto de dados estiver disponível em uma URL, inclua a URL no seu código e carregue-o diretamente.\n",
        "\n",
        "**2. Crie uma instância para MNISTDataset, MNISTModel, MNISTExperiment.**\n",
        "\n",
        "**3. Execute as funções:**\n",
        "\n",
        "* ***experiment.run:*** Executa o experimento centralizado."
      ],
      "metadata": {
        "id": "mrnii3iLxx1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import flautim as fl\n",
        "import flautim.metrics as flm\n",
        "import MNISTDataset, MNISTModel, MNISTExperiment\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "\n",
        "FM_NORMALIZATION = ((0.1307,), (0.3081,))\n",
        "EVAL_TRANSFORMS = Compose([ToTensor(), Normalize(*FM_NORMALIZATION)])\n",
        "TRAIN_TRANSFORMS = Compose(\n",
        "    [\n",
        "        RandomCrop(28, padding=4),\n",
        "        RandomHorizontalFlip(),\n",
        "        ToTensor(),\n",
        "        Normalize(*FM_NORMALIZATION),\n",
        "    ]\n",
        ")\n",
        "\n",
        "DATASET=\"ylecun/mnist\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    context = fl.init()\n",
        "\n",
        "    fl.log(f\"Flautim inicializado!!!\")\n",
        "\n",
        "    partition = load_dataset(\"zalando-datasets/fashion_mnist\")\n",
        "\n",
        "    dataset = MNISTDataset.MNISTDataset(FM_NORMALIZATION, EVAL_TRANSFORMS, TRAIN_TRANSFORMS, partition, split_data=False, batch_size = 32, shuffle = False, num_workers = 0)\n",
        "\n",
        "    model = MNISTModel.MNISTModel(context, num_classes = 10)\n",
        "\n",
        "    fl.log(f\"Modelo criado!!!\")\n",
        "\n",
        "    experiment = MNISTExperiment.MNISTExperiment(model, dataset,  context)\n",
        "\n",
        "    fl.log(f\"Experimento criado!!!\")\n",
        "\n",
        "    # Exemplo de métrica implementada pelo usuário\n",
        "    def accuracy_2(y, y_hat):\n",
        "        y = np.asarray(y)\n",
        "        y_hat = np.asarray(y_hat)\n",
        "        return np.mean(y == y_hat)\n",
        "\n",
        "    # Adiciona a métrica ao módulo de métricas\n",
        "    flm.Metrics.accuracy_2 = accuracy_2\n",
        "\n",
        "    fl.log(f\"Métrica adicionada!!!\")\n",
        "\n",
        "    experiment.run(metrics = {'ACCURACY': flm.Metrics.accuracy, 'ACCURACY_2': flm.Metrics.accuracy_2})"
      ],
      "metadata": {
        "id": "F19b12aGxxTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Passo 3.2: Experimento federado\n",
        "**Implementando a Classe MNISTExperiment**\n",
        "\n",
        "No código abaixo, criamos a classe MNISTExperiment no modo federado com seus métodos training_loop e evaluation_loop para treinar e testar a rede neural. Esses métodos retornam o valor da função de perda e a acurácia de treinamento e de validação."
      ],
      "metadata": {
        "id": "AEABgvWrGxNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flautim.pytorch.federated.Experiment import Experiment\n",
        "import flautim as fl\n",
        "import flautim.metrics as flm\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "\n",
        "class MNISTExperiment(Experiment):\n",
        "    def __init__(self, model, dataset, context, **kwargs):\n",
        "        super(MNISTExperiment, self).__init__(model, dataset, context, **kwargs)\n",
        "\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "        self.lr = kwargs.get('lr', 0.01)\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        self.epochs = kwargs.get('epochs', 30)\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Exemplo de métrica implementada pelo usuário\n",
        "    def accuracy_2(y, y_hat):\n",
        "        y = np.asarray(y)\n",
        "        y_hat = np.asarray(y_hat)\n",
        "        return np.mean(y == y_hat)\n",
        "\n",
        "    # Adiciona a métrica ao módulo de métricas\n",
        "    flm.Metrics.accuracy_2 = accuracy_2\n",
        "\n",
        "    def training_loop(self, data_loader):\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.train()\n",
        "\n",
        "        correct, running_loss = 0.0, 0.0\n",
        "\n",
        "        yhat, y_real = [], []\n",
        "\n",
        "        for batch in data_loader:\n",
        "            images = batch[\"image\"]\n",
        "            labels = batch[\"label\"]\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(images.to(self.device))\n",
        "            loss = self.criterion(outputs, labels.to(self.device))\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            correct += (torch.max(outputs, 1)[1].cpu() == labels.cpu()).sum().item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            yhat.append(predicted.detach().cpu())\n",
        "            y_real.append(labels.detach().cpu())\n",
        "\n",
        "        accuracy = flm.Metrics.accuracy(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        accuracy_2 = flm.Metrics.accuracy_2(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        avg_trainloss = running_loss / len(data_loader)\n",
        "        return float(avg_trainloss), {'ACCURACY': accuracy, \"ACCURACY_2\": accuracy_2}\n",
        "\n",
        "    def validation_loop(self, data_loader):\n",
        "\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        correct, loss = 0, 0.0\n",
        "\n",
        "        yhat, y_real = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in data_loader:\n",
        "                images = batch[\"image\"].to(self.device)\n",
        "                labels = batch[\"label\"].to(self.device)\n",
        "                outputs = self.model(images)\n",
        "                loss += self.criterion(outputs, labels).item()\n",
        "                correct += (torch.max(outputs, 1)[1].cpu() == labels.cpu()).sum().item()\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                yhat.append(predicted.detach().cpu())\n",
        "                y_real.append(labels.detach().cpu())\n",
        "\n",
        "        accuracy = flm.Metrics.accuracy(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        accuracy_2 = flm.Metrics.accuracy_2(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        loss = loss / len(data_loader)\n",
        "\n",
        "        return float(loss), {'ACCURACY': accuracy, \"ACCURACY_2\": accuracy_2}"
      ],
      "metadata": {
        "id": "zCOysvCsG5qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementando o run.py para realização de um experimento federado**\n",
        "\n",
        "**1. Upload do Conjunto de Dados:**\n",
        "\n",
        "* *Arquivo Local:* Se o seu conjunto de dados for um arquivo (por exemplo, CSV, NPZ, etc.), faça o upload para a plataforma e carregue-o usando o caminho ./data/nomedoarquivo.\n",
        "\n",
        "* *URL:* Se o conjunto de dados estiver disponível em uma URL, inclua a URL no seu código e carregue-o diretamente.\n",
        "\n",
        "**2. Separação dos dados por cliente:**\n",
        "\n",
        "* Para simular 4 clientes, divida os dados em 4 partes.\n",
        "\n",
        "**3. Crie uma instância para MNISTDataset, MNISTModel, MnistExperiment.**\n",
        "\n",
        "**4. Execute as funções:**\n",
        "* ***generate_server_fn:*** Cria a estratégia para o aprendizado federado\n",
        "* ***generate_client_fn:*** Gera o modelo e o dataset de cada cliente.\n",
        "* ***evaluate_fn:*** Avalia o modelo global usando o dataset de um dos clientes.\n",
        "* ***run_federated:*** Executa o experimento federado."
      ],
      "metadata": {
        "id": "QO279cKP1BNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flautim.pytorch.common import run_federated, weighted_average\n",
        "from flautim.pytorch import Model, Dataset\n",
        "from flautim.pytorch.federated import Experiment\n",
        "import MNISTDataset, MNISTModel, MnistExperiment\n",
        "import flautim as fl\n",
        "from flwr_datasets import FederatedDataset\n",
        "from flwr_datasets.partitioner import DirichletPartitioner\n",
        "from flwr.common import Context, ndarrays_to_parameters\n",
        "import flwr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from flwr.server import ServerConfig, ServerAppComponents\n",
        "from datasets import load_dataset\n",
        "\n",
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomCrop,\n",
        "    RandomHorizontalFlip,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "\n",
        "FM_NORMALIZATION = ((0.1307,), (0.3081,))\n",
        "EVAL_TRANSFORMS = Compose([ToTensor(), Normalize(*FM_NORMALIZATION)])\n",
        "TRAIN_TRANSFORMS = Compose(\n",
        "    [\n",
        "        RandomCrop(28, padding=4),\n",
        "        RandomHorizontalFlip(),\n",
        "        ToTensor(),\n",
        "        Normalize(*FM_NORMALIZATION),\n",
        "    ]\n",
        ")\n",
        "\n",
        "DATASET = \"zalando-datasets/fashion_mnist\"\n",
        "NUM_PARTITIONS = 30\n",
        "\n",
        "def fit_config(server_round: int):\n",
        "    \"\"\"Return training configuration dict for each round.\n",
        "\n",
        "    Perform two rounds of training with one local epoch, increase to two local\n",
        "    epochs afterwards.\n",
        "    \"\"\"\n",
        "    config = {\n",
        "        \"server_round\": server_round,  # The current round of federated learning\n",
        "    }\n",
        "    return config\n",
        "\n",
        "\n",
        "\n",
        "def generate_server_fn(context, eval_fn, **kwargs):\n",
        "\n",
        "    def create_server_fn(context_flwr:  Context):\n",
        "\n",
        "        net = MNISTModel.MNISTModel(context, num_classes = 10, suffix = 0)\n",
        "        params = ndarrays_to_parameters(net.get_parameters())\n",
        "\n",
        "        strategy = flwr.server.strategy.FedAvg(\n",
        "                          initial_parameters=params,\n",
        "                          evaluate_metrics_aggregation_fn=weighted_average,\n",
        "                          fraction_fit=0.1,  # 10% clients sampled each round to do fit()\n",
        "                          fraction_evaluate=0.5,  # 50% clients sample each round to do evaluate()\n",
        "                          evaluate_fn=eval_fn,\n",
        "                          on_fit_config_fn = fit_config,\n",
        "                          on_evaluate_config_fn = fit_config\n",
        "                          )\n",
        "        num_rounds = 30\n",
        "        config = ServerConfig(num_rounds=num_rounds)\n",
        "\n",
        "        return ServerAppComponents(config=config, strategy=strategy)\n",
        "    return create_server_fn\n",
        "\n",
        "def generate_client_fn(context):\n",
        "\n",
        "    def create_client_fn(context_flwr:  Context):\n",
        "\n",
        "        global fds\n",
        "\n",
        "        cid = int(context_flwr.node_config[\"partition-id\"])\n",
        "\n",
        "        partition = fds.load_partition(cid)\n",
        "\n",
        "        model = MNISTModel.MNISTModel(context, num_classes = 10, suffix = cid)\n",
        "\n",
        "        dataset = MNISTDataset.MNISTDataset(FM_NORMALIZATION, EVAL_TRANSFORMS, TRAIN_TRANSFORMS, partition, batch_size = 32, shuffle = False, num_workers = 0)\n",
        "\n",
        "        return MNISTExperiment.MNISTExperiment(model, dataset,  context).to_client()\n",
        "\n",
        "    return create_client_fn\n",
        "\n",
        "\n",
        "def evaluate_fn(context):\n",
        "    def fn(server_round, parameters, config):\n",
        "        \"\"\"This function is executed by the strategy it will instantiate\n",
        "        a model and replace its parameters with those from the global model.\n",
        "        The, the model will be evaluate on the test set (recall this is the\n",
        "        whole MNIST test set).\"\"\"\n",
        "\n",
        "        global FM_NORMALIZATION, EVAL_TRANSFORMS, TRAIN_TRANSFORMS, DATASET\n",
        "        global fds\n",
        "\n",
        "        model = MNISTModel.MNISTModel(context, num_classes = 10, suffix = \"FL-Global\")\n",
        "        model.set_parameters(parameters)\n",
        "\n",
        "        partition = fds.load_partition(0)\n",
        "\n",
        "        dataset = MNISTDataset.MNISTDataset(FM_NORMALIZATION, EVAL_TRANSFORMS, TRAIN_TRANSFORMS, partition, batch_size = 32, shuffle = False, num_workers = 0)\n",
        "        dataset.test_partition = load_dataset(DATASET)[\"test\"]\n",
        "\n",
        "        experiment = MNISTExperiment.MNISTExperiment(model, dataset, context)\n",
        "\n",
        "        config[\"server_round\"] = server_round\n",
        "\n",
        "        loss, _, return_dic = experiment.evaluate(parameters, config)\n",
        "\n",
        "        return loss, return_dic\n",
        "\n",
        "    return fn\n",
        "\n",
        "partitioner = DirichletPartitioner(\n",
        "            num_partitions=NUM_PARTITIONS,\n",
        "            partition_by=\"label\",\n",
        "            alpha=1.0,\n",
        "            seed=42,\n",
        "        )\n",
        "fds = FederatedDataset(\n",
        "            dataset=DATASET,\n",
        "            partitioners={\"train\": partitioner},\n",
        "        )\n",
        "\n",
        "fds.load_partition(0)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    context = fl.init()\n",
        "\n",
        "    fl.log(f\"Flautim inicializado!!!\")\n",
        "\n",
        "\n",
        "    client_fn_callback = generate_client_fn(context)\n",
        "    evaluate_fn_callback = evaluate_fn(context)\n",
        "    server_fn_callback = generate_server_fn(context, eval_fn = evaluate_fn_callback)\n",
        "\n",
        "    fl.log(f\"Experimento criado!!!\")\n",
        "\n",
        "    run_federated(client_fn_callback, server_fn_callback, num_clients = NUM_PARTITIONS)"
      ],
      "metadata": {
        "id": "25KUsCbg07mp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
