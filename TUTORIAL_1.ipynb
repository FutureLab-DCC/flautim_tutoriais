{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table style=\"margin: auto; background-color: white;\">\n",
        "    <tr>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1R6PphT9jmd2vikODFPf6cW54QtZ29o2a' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1R6PphT9jmd2vikODFPf6cW54QtZ29o2a' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1R6PphT9jmd2vikODFPf6cW54QtZ29o2a' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "    </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "XqLZVV10-uOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TUTORIAL 1\n",
        "\n",
        "Bem-vindo! Neste tutorial você aprenderá sobre a interface de programação da plataforma **Flautim** e também como montar um experimento simples de classificação usando o dataset [IRIS](https://archive.ics.uci.edu/dataset/53/iris).\n",
        "\n",
        "O código desse tutorial pode ser acessado em: [clique aqui.](https://github.com/FutureLab-DCC/flautim_tutoriais/tree/main/TUTORIAL_1)\n",
        "\n",
        "\n",
        "Vamos começar entendendo a interface de programação da **Flautim**, representada na figura abaixo. A **Flautim_api** é uma biblioteca modularizada que facilita a realização de experimentos de aprendizado de máquina, seja convencional/centralizado ou federado.\n",
        "\n",
        "Todo projeto **Flautim** precisa herdar essa biblioteca, que contém submódulos específicos para diferentes tecnologias (por exemplo, submódulos para PyTorch, TensorFlow, etc). Neste tutorial usaremos o submódulo para PyTorch.\n",
        "\n",
        "<div style=\"text-align: center;\"> <table style=\"margin: auto;\"> <tr> <td> <img src='https://drive.google.com/uc?export=view&id=1QOI4jWrwS979xhW_wlGzkPa2bMA-giuc' alt=\"Interface da plataforma Flautim\" width=\"800\" /> </td> </tr> </table> </div>\n",
        "\n",
        "\n",
        "Dentro de cada submódulo existem três componentes principais (classes):\n",
        "\n",
        "**1. Dataset:** é utilizado para representar os dados do experimento. Esta classe pode ser reutilizada em diversos experimentos e com diferentes modelos, sendo o componente mais versátil e reutilizável. Os usuários podem importar os dados de diversas fontes, como arquivos locais ou bases de dados online, desde que a classe Dataset seja herdada.\n",
        "\n",
        "**2. Model:** representa qualquer conjunto de parâmetros treináveis dentro do projeto. Ela permite a aplicação de técnicas de aprendizado de máquina por meio de treinamento desses parâmetros. No caso de PyTorch, a classe herda a nn.Module, que define a estrutura e os parâmetros treináveis do modelo.\n",
        "\n",
        "**3. Experiment:** define o ciclo de treinamento e validação. Existem dois tipos principais de experimentos: o experimento centralizado, que segue o fluxo\n",
        "convencional de aprendizado de máquina, e o experimento federado, adaptado para\n",
        "aprendizado federado. Esta classe inclui duas funções principais, um loop de\n",
        "treinamento e um loop de validação, que realizam a atualização dos parâmetros e\n",
        "cálculo das métricas de custo, respectivamente.\n",
        "\n",
        "Além desses três componentes principais, há também um módulo chamado Common. Este módulo fornece acesso a classes essenciais para o gerenciamento de dados e monitoramento do treinamento.\n",
        "\n",
        "\n",
        "Com essa visão geral, você está pronto para começar montar seus próprios experimentos. Vamos ao passo a passo!"
      ],
      "metadata": {
        "id": "5HacTzoG_xm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Passo 1: Criando o dataset que será usado no experimento\n",
        "\n",
        "Um conjunto de dados no Flautim é acessado por um arquivo .py que deve conter uma classe que herda de Dataset.\n",
        "\n",
        "**Exemplo: Implementando a Classe IRISDataset**\n",
        "\n",
        "O código abaixo implementa uma classe IRISDataset utilizando o dataset IRIS para resolver um problema de classificação."
      ],
      "metadata": {
        "id": "ChJrIZCYeTeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flautim2.pytorch.Dataset import Dataset\n",
        "import torch\n",
        "import copy\n",
        "\n",
        "class IRISDataset(Dataset):\n",
        "\n",
        "    def __init__(self, file, **kwargs):\n",
        "        super(IRISDataset, self).__init__(name = \"IRIS\", **kwargs)\n",
        "\n",
        "        # Defina o que são features e targets\n",
        "        self.features = file.iloc[:, 0:4].values\n",
        "        self.target = file.iloc[:, 4].values\n",
        "\n",
        "        # Número de amostras para teste\n",
        "        self.test_size = int(0.2 * len(file))\n",
        "\n",
        "        # Defina o tipo do tensor de entrada e de saída.\n",
        "        self.xdtype = torch.float32\n",
        "        self.ydtype = torch.int64\n",
        "\n",
        "        # batch_size\n",
        "        self.batch_size = 10\n",
        "\n",
        "        # shuffle\n",
        "        self.shuffle = True\n",
        "\n",
        "        # num_workers\n",
        "        self.num_workers = 1\n",
        "\n",
        "    def train(self) -> Dataset:\n",
        "        # Separação das amostras para treino\n",
        "        self.features = self.features[:-self.test_size]\n",
        "        self.target = self.target[:-self.test_size]\n",
        "        return copy.deepcopy(self)\n",
        "\n",
        "    def validation(self) -> Dataset:\n",
        "        # Separação das amostras para validação\n",
        "        self.features = self.features[-self.test_size:]\n",
        "        self.target = self.target[-self.test_size:]\n",
        "        return copy.deepcopy(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.features[idx], dtype=torch.float32), torch.LongTensor([self.target[idx]])"
      ],
      "metadata": {
        "id": "Hz5LGFkzc9CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Passo 2: Criando o modelo que será usado no experimento\n",
        "\n",
        "Agora, vamos criar a classe que implementa o modelo. Essa classe deve herdar da classe Model.\n",
        "\n",
        "\n",
        "**Exemplo: Implementando a Classe IRISModel**\n",
        "\n",
        "A classe IRISModel implementa uma rede neural com 4 entradas e 3 saídas."
      ],
      "metadata": {
        "id": "6EbqONNwmV3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X51g0zkU-ikV"
      },
      "outputs": [],
      "source": [
        "from flautim2.pytorch.Model import Model\n",
        "import torch\n",
        "\n",
        "class IRISModel(Model):\n",
        "    def __init__(self, context, num_classes = 3, **kwargs):\n",
        "        super(IRISModel, self).__init__(context, name = \"IRIS-NN\", **kwargs)\n",
        "\n",
        "        # Rede neural com 4 entradas e 3 saídas\n",
        "        self.c1 = torch.nn.Linear(4, 10)\n",
        "        self.c2 = torch.nn.dropout(0.1)\n",
        "        self.c2 = torch.nn.Linear(10, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.c1(x))\n",
        "        x = torch.relu(self.c2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Passo 3: Criando o experimento\n",
        "\n",
        "Por fim, será criado o experimento, isto é, uma classe que implementa os loops de treinamento e validação do modelo IRISModel no dataset IRISDataset. Para isso, precisamos criar dois arquivos .py, o run.py (que deve ter obrigatoriamente esse nome) e o .py responsável por implementar o experimento, descritos a seguir:\n",
        "\n",
        "**1. Arquivo run.py:**\n",
        "\n",
        "* Esse arquivo é o ponto de entrada de todo experimento Flautim, pois é ele\n",
        "que deve iniciar a classe do experimento e também um modelo e um Dataset.\n",
        "\n",
        "**2. Arquivo .py do experimento:**\n",
        "\n",
        "* Esse arquivo deve conter uma classe que implemente os métodos de treinamento (training_loop) e validação (evaluation_loop) do modelo. Essa classe deve herdar da classe Experiment.\n",
        "\n",
        "Esse tutorial cobrirá dois tipos de experimentos, um experimento centralizado e outro descentralizado. Portanto, o passo 3 será dividido entre esses dois cenários."
      ],
      "metadata": {
        "id": "_TF_5DkrmXwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Passo 3.1: Experimento centralizado\n",
        "\n",
        "**Implementando a Classe IRISExperiment**\n",
        "\n",
        "No código abaixo, a classe IRISExperiment foi criada no modo centralizado com seus métodos training_loop e evaluation_loop para treinar e testar a rede neural. Esses métodos retornam o valor da função de perda e a acurácia de treinamento e de validação."
      ],
      "metadata": {
        "id": "pnc_5yyYxJUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Chamar flautim.pytorch.centralized.Experiment para experimento centralizado\n",
        "from flautim.pytorch.centralized.Experiment import Experiment\n",
        "\n",
        "import flautim2 as fl\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "\n",
        "class IRISExperiment(Experiment):\n",
        "    def __init__(self, model, dataset, context, **kwargs):\n",
        "        super(IRISExperiment, self).__init__(model, dataset, context, **kwargs)\n",
        "\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
        "        self.epochs = kwargs.get('epochs', 30)\n",
        "\n",
        "\n",
        "    def training_loop(self, data_loader):\n",
        "        self.model.train()\n",
        "        error_loss = 0.0\n",
        "        yhat, y_real = [], []\n",
        "\n",
        "        for X, y in data_loader:\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(X)\n",
        "            loss = self.criterion(outputs, y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            error_loss += loss.cpu().item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            yhat.append(predicted.detach().cpu())\n",
        "            y_real.append(y.detach().cpu())\n",
        "\n",
        "        accuracy = self.metrics.ACCURACY(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        accuracy_2 = self.metrics.ACCURACY_2(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        error_loss = error_loss / len(data_loader)\n",
        "\n",
        "        return float(error_loss), {'ACCURACY': accuracy, 'ACCURACY_2': accuracy_2}\n",
        "\n",
        "    def validation_loop(self, data_loader):\n",
        "        error_loss = 0.0\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in data_loader:\n",
        "                outputs = self.model(X)\n",
        "                error_loss += self.criterion(outputs, y).item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                yhat.append(predicted.detach().cpu())\n",
        "            y_real.append(y.detach().cpu())\n",
        "\n",
        "        accuracy = self.metrics.ACCURACY(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        accuracy_2 = self.metrics.ACCURACY_2(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        error_loss = error_loss / len(data_loader)\n",
        "\n",
        "        return float(error_loss), {'ACCURACY': accuracy, 'ACCURACY_2': accuracy_2}"
      ],
      "metadata": {
        "id": "md90drlNmZqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementando o run.py para realização de um experimento centralizado**\n",
        "\n",
        "**1. Upload do Conjunto de Dados:**\n",
        "\n",
        "* *Arquivo Local:* Se o seu conjunto de dados for um arquivo (por exemplo, CSV, NPZ, etc.), faça o upload para a plataforma e carregue-o usando o caminho ./data/nomedoarquivo.\n",
        "\n",
        "* *URL:* Se o conjunto de dados estiver disponível em uma URL, inclua a URL no seu código e carregue-o diretamente.\n",
        "\n",
        "**2. Crie uma instância para IRISDataset, IRISModel, IRISExperiment.**\n",
        "\n",
        "**3. Execute as funções:**\n",
        "\n",
        "* ***experiment.run:*** Executa o experimento centralizado."
      ],
      "metadata": {
        "id": "mrnii3iLxx1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import flautim as fl\n",
        "import IRISDataset, IRISModel, IRISExperiment\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import flautim2.metrics as flm\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    context = fl.init()\n",
        "\n",
        "    fl.log(f\"Flautim2 inicializado!!!\")\n",
        "\n",
        "    # Carregue os dados usando dataset próprio\n",
        "    iris = pd.read_csv(\"./data/iris.csv\", header=None)\n",
        "\n",
        "    # Carregue os dados usando uma URL\n",
        "    iris = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None)\n",
        "    iris.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "    iris['class'] = pd.factorize(iris['class'])[0]\n",
        "\n",
        "    # Embaralhe os dados\n",
        "    file = iris.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    dataset = IRISDataset.IRISDataset(file, batch_size = 10, shuffle = False, num_workers = 0)\n",
        "\n",
        "    model = IRISModel.IRISModel(context)\n",
        "\n",
        "    experiment = IRISExperiment.IRISExperiment(model, dataset, context)\n",
        "\n",
        "    # Exemplo de métrica implementada pelo usuário\n",
        "    def accuracy_2(y, y_hat):\n",
        "        y = np.asarray(y)\n",
        "        y_hat = np.asarray(y_hat)\n",
        "        return np.mean(y == y_hat)\n",
        "\n",
        "    # Adiciona a métrica ao módulo de métricas\n",
        "    flm.Metrics.accuracy_2 = accuracy_2\n",
        "\n",
        "    experiment.run(metrics = {'ACCURACY': flm.Metrics.accuracy, 'ACCURACY_2': flm.Metrics.accuracy_2})"
      ],
      "metadata": {
        "id": "F19b12aGxxTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Passo 3.2: Experimento federado\n",
        "**Implementando a Classe IRISExperiment**\n",
        "\n",
        "No código abaixo, criamos a classe IRISExperiment no modo federado com seus métodos training_loop e evaluation_loop para treinar e testar a rede neural. Esses métodos retornam o valor da função de perda e a acurácia de treinamento e de validação."
      ],
      "metadata": {
        "id": "AEABgvWrGxNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Chamar flautim.pytorch.federated.Experiment para experimento federado\n",
        "from flautim.pytorch.federated.Experiment import Experiment\n",
        "\n",
        "import flautim.metrics as flm\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "\n",
        "class IRISExperiment(Experiment):\n",
        "    def __init__(self, model, dataset, context, **kwargs):\n",
        "        super(IRISExperiment, self).__init__(model, dataset, context, **kwargs)\n",
        "\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
        "        self.epochs = kwargs.get('epochs', 20)\n",
        "\n",
        "    # Exemplo de métrica implementada pelo usuário\n",
        "    def accuracy_2(y, y_hat):\n",
        "        y = np.asarray(y)\n",
        "        y_hat = np.asarray(y_hat)\n",
        "        return np.mean(y == y_hat)\n",
        "\n",
        "    # Adiciona a métrica ao módulo de métricas\n",
        "    flm.Metrics.accuracy_2 = accuracy_2\n",
        "\n",
        "\n",
        "    def training_loop(self, data_loader):\n",
        "\n",
        "        self.model.train()\n",
        "        error_loss = 0.0\n",
        "        yhat, y_real = [], []\n",
        "\n",
        "        for X, y in data_loader:\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(X)\n",
        "            loss = self.criterion(outputs, y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            error_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            yhat.append(predicted.detach().cpu())\n",
        "            y_real.append(y.detach().cpu())\n",
        "\n",
        "        accuracy = flm.Metrics.accuracy(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        accuracy_2 = flm.Metrics.accuracy_2(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        error_loss = error_loss / len(data_loader)\n",
        "        return error_loss, {\"ACCURACY\": accuracy, \"ACCURACY_2\": accuracy_2}\n",
        "\n",
        "    def validation_loop(self, data_loader):\n",
        "        error_loss = 0.0\n",
        "        yhat, y_real = [], []\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in data_loader:\n",
        "                outputs = self.model(X)\n",
        "                error_loss += self.criterion(outputs, y).item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                yhat.append(predicted.detach().cpu())\n",
        "                y_real.append(y.detach().cpu())\n",
        "\n",
        "        accuracy = flm.Metrics.accuracy(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        accuracy_2 = flm.Metrics.accuracy_2(torch.cat(yhat).numpy(), torch.cat(y_real).numpy())\n",
        "        error_loss = error_loss / len(data_loader)\n",
        "        return error_loss, {\"ACCURACY\": accuracy, \"ACCURACY_2\": accuracy_2}"
      ],
      "metadata": {
        "id": "zCOysvCsG5qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementando o run.py para realização de um experimento federado**\n",
        "\n",
        "**1. Upload do Conjunto de Dados:**\n",
        "\n",
        "* *Arquivo Local:* Se o seu conjunto de dados for um arquivo (por exemplo, CSV, NPZ, etc.), faça o upload para a plataforma e carregue-o usando o caminho ./data/nomedoarquivo.\n",
        "\n",
        "* *URL:* Se o conjunto de dados estiver disponível em uma URL, inclua a URL no seu código e carregue-o diretamente.\n",
        "\n",
        "**2. Separação dos dados por cliente:**\n",
        "\n",
        "* Para simular 4 clientes, divida os dados em 4 partes.\n",
        "\n",
        "**3. Crie uma instância para IRISDataset, IRISModel, IRISExperiment.**\n",
        "\n",
        "**4. Execute as funções:**\n",
        "* ***generate_server_fn:*** Cria a estratégia para o aprendizado federado\n",
        "* ***generate_client_fn:*** Gera o modelo e o dataset de cada cliente.\n",
        "* ***evaluate_fn:*** Avalia o modelo global usando o dataset de um dos clientes.\n",
        "* ***run_federated_2:*** Executa o experimento federado."
      ],
      "metadata": {
        "id": "QO279cKP1BNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flautim.pytorch.common import run_federated, weighted_average\n",
        "from flautim.pytorch import Model, Dataset\n",
        "from flautim.pytorch.federated import Experiment\n",
        "import IRISDataset, IRISModel, IRISExperiment\n",
        "import flautim as fl\n",
        "import flwr\n",
        "from flwr.common import Context, ndarrays_to_parameters\n",
        "from flwr.server import ServerConfig, ServerAppComponents\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def fit_config(server_round: int):\n",
        "    \"\"\"Return training configuration dict for each round.\n",
        "\n",
        "    Perform two rounds of training with one local epoch, increase to two local\n",
        "    epochs afterwards.\n",
        "    \"\"\"\n",
        "    config = {\n",
        "        \"server_round\": server_round,  # The current round of federated learning\n",
        "    }\n",
        "    return config\n",
        "\n",
        "\n",
        "\n",
        "def generate_server_fn(context, eval_fn, **kwargs):\n",
        "\n",
        "    def create_server_fn(context_flwr:  Context):\n",
        "\n",
        "        net = IRISModel.IRISModel(context, suffix = 0)\n",
        "        params = ndarrays_to_parameters(net.get_parameters())\n",
        "\n",
        "        strategy = flwr.server.strategy.FedAvg(\n",
        "                          initial_parameters=params,\n",
        "                          evaluate_metrics_aggregation_fn=weighted_average,\n",
        "                          fraction_fit=0.2,  # 10% clients sampled each round to do fit()\n",
        "                          fraction_evaluate=0.5,  # 50% clients sample each round to do evaluate()\n",
        "                          evaluate_fn=eval_fn,\n",
        "                          on_fit_config_fn = fit_config,\n",
        "                          on_evaluate_config_fn = fit_config\n",
        "                          )\n",
        "        num_rounds = 20\n",
        "        config = ServerConfig(num_rounds=num_rounds)\n",
        "\n",
        "        return ServerAppComponents(config=config, strategy=strategy)\n",
        "    return create_server_fn\n",
        "\n",
        "def generate_client_fn(context, files):\n",
        "\n",
        "    def create_client_fn(context_flwr:  Context):\n",
        "\n",
        "        cid = int(context_flwr.node_config[\"partition-id\"])\n",
        "        file = int(cid)\n",
        "        model = IRISModel.IRISModel(context, suffix = cid)\n",
        "        dataset = IRISDataset.IRISDataset(files[file], batch_size = 10, shuffle = False, num_workers = 0)\n",
        "\n",
        "        return IRISExperiment.IRISExperiment(model, dataset, context).to_client()\n",
        "\n",
        "    return create_client_fn\n",
        "\n",
        "\n",
        "def evaluate_fn(context, files):\n",
        "    def fn(server_round, parameters, config):\n",
        "        \"\"\"This function is executed by the strategy it will instantiate\n",
        "        a model and replace its parameters with those from the global model.\n",
        "        The, the model will be evaluate on the test set (recall this is the\n",
        "        whole MNIST test set).\"\"\"\n",
        "\n",
        "        model = IRISModel.IRISModel(context, suffix = \"FL-Global\")\n",
        "        model.set_parameters(parameters)\n",
        "\n",
        "        dataset = IRISDataset.IRISDataset(files[0], batch_size = 10, shuffle = False, num_workers = 0)\n",
        "\n",
        "        experiment = IRISExperiment.IRISExperiment(model, dataset, context)\n",
        "\n",
        "        config[\"server_round\"] = server_round\n",
        "\n",
        "        loss, _, return_dic = experiment.evaluate(parameters, config)\n",
        "\n",
        "        return loss, return_dic\n",
        "\n",
        "    return fn\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    context = fl.init()\n",
        "    fl.log(f\"Flautim2 inicializado!!!\")\n",
        "\n",
        "    num_clientes = 2\n",
        "\n",
        "    iris = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", header=None)\n",
        "    iris.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "    iris['class'] = pd.factorize(iris['class'])[0]\n",
        "\n",
        "\n",
        "    iris = iris.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    files = np.array_split(iris, num_clientes)\n",
        "\n",
        "    client_fn_callback = generate_client_fn(context, files)\n",
        "    evaluate_fn_callback = evaluate_fn(context, files)\n",
        "    server_fn_callback = generate_server_fn(context, eval_fn = evaluate_fn_callback)\n",
        "\n",
        "    run_federated(client_fn_callback, server_fn_callback)"
      ],
      "metadata": {
        "id": "25KUsCbg07mp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}